\relax 
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control}
\abx@aux@refcontext{anyt/global//global/global}
\citation{Rupp}
\abx@aux@cite{0}{Rupp}
\abx@aux@segm{0}{0}{Rupp}
\citation{Rupp}
\abx@aux@cite{0}{Rupp}
\abx@aux@segm{0}{0}{Rupp}
\citation{programming_guide}
\abx@aux@cite{0}{programming_guide}
\abx@aux@segm{0}{0}{programming_guide}
\citation{programming_guide}
\abx@aux@cite{0}{programming_guide}
\abx@aux@segm{0}{0}{programming_guide}
\citation{Intel4004}
\abx@aux@cite{0}{Intel4004}
\abx@aux@segm{0}{0}{Intel4004}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}From single-core CPUs over multi-core CPUs to GPUs}{3}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_cpu_trend}{{\caption@xref {fig_cpu_trend}{ on input line 3}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Trend in performance of CPUs over the last 40 years. Data up to the year 2010 collected by M. Holowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten. Data after the year 2010 collected by K. Rupp. Figure taken from \blx@tocontentsinit {0}\cite {Rupp}. \relax }}{4}{}\protected@file@percent }
\newlabel{fig_tree_reduction}{{\caption@xref {fig_tree_reduction}{ on input line 14}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Comparison of the basic architectural differences of a CPU and a GPU. In green the random access memory (RAM), in yellow the cache, in blue the flow control unit and in red the arithmetic-logic unit (ALU). Both designs feature a RAM that all threads have access to. The GPU is split into many small "CPUs" (vertical groups) with their own flow control and cache. These are called streaming multiprocessors (SMPs or SMs). Generally the cache hierachry is more complicated as depicted in the figure. However, the defining property here is that there exists a non-global cache level that is assigned to a group of ALUs, namely the SMs. Figure inspired by \blx@tocontentsinit {0}\cite {programming_guide}. \relax }}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}What is CUDA}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Tree Reduction on GPUs}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The importance of reductions}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The tree reduction algorithm}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Example of a tree reduction of ten elements. Note how there is a leftover after the first reduction. These are usually handled by zeropadding (i.e. adding zeros) after each step to make the number of elements divisible by 2. \relax }}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Naive implementation with CUDA}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Serial tree reduction on a CPU}{7}{}\protected@file@percent }
\newlabel{fig_naive_reduction}{{\caption@xref {fig_naive_reduction}{ on input line 45}}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Sketch of the implementation of the tree reduction algorithm presented in this section. The cells denote entries in the data array. Two arrows pointing into a cell denote addition of the source cells values and writing of the result into the target cell. The final result is found in the first cell of the array. \relax }}{8}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/host\textunderscore reduce.c}{8}{}\protected@file@percent }
\citation{programming_guide}
\abx@aux@cite{0}{programming_guide}
\abx@aux@segm{0}{0}{programming_guide}
\@writefile{lol}{\contentsline {lstlisting}{code/host\textunderscore reduce\textunderscore swapped\textunderscore loops.c}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Tree reduction in CUDA for small arrays}{9}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/device\textunderscore small\textunderscore kernel0.cu}{9}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/shared\textunderscore memory\textunderscore line.cu}{10}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel\textunderscore invokation.cu}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  Depicted is the schematic tree reduction in block structure for an array of length 16 and 4 threads per block. The first step requires 4 blocks and the second 1 block. The blocks run on the SMs of the GPU. The first step allows for four SMs and a total of 16 physical threads to be used in parallel. Between the two steps the output array is used as new input array. The second step can only make use of a single SM. \relax }}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Tree reduction in CUDA for arbitrary array sizes}{11}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/device\textunderscore full\textunderscore kernel0.cu}{12}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel\textunderscore invokation\textunderscore full.cu}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{13}{}\protected@file@percent }
\citation{Harris}
\abx@aux@cite{0}{Harris}
\abx@aux@segm{0}{0}{Harris}
\citation{Harris}
\abx@aux@cite{0}{Harris}
\abx@aux@segm{0}{0}{Harris}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Optimizations}{14}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Starting point: The naive kernel}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Divergent warps}{15}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/divergent\textunderscore warps\textunderscore worst.cu}{15}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/divergent\textunderscore warps\textunderscore fixed.cu}{15}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel0\textunderscore ifstatement.cu}{16}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel1\textunderscore ifstatement.cu}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Memory bank conflicts}{16}{}\protected@file@percent }
\newlabel{fig_coalesced_reduction}{{\caption@xref {fig_coalesced_reduction}{ on input line 87}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  Depicted is the same algorithm as in fig. \caption@xref  {fig_naive_reduction}{ on input line 45}\hbox {} for 8 elements. However, this time the results of the addition of two cells is written into memory, such that the data stays contiguous. Note that one needs to be careful not to introduce an additional race condition when accessing the elements. \relax }}{17}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel2\textunderscore forloop.cu}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Idle threads after load}{17}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel2\textunderscore load\textunderscore and\textunderscore add.cu}{18}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel3\textunderscore load\textunderscore and\textunderscore add.cu}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Implicit synchronisation within a warp}{18}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel4\textunderscore subroutine.cu}{18}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{code/kernel4\textunderscore forloop.cu}{18}{}\protected@file@percent }
\citation{Harris}
\abx@aux@cite{0}{Harris}
\abx@aux@segm{0}{0}{Harris}
\newlabel{tab_reduce_performance}{{\caption@xref {tab_reduce_performance}{ on input line 174}}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Listed are the bandwidths in GB/s that the G80 (CUDA version 1.1, max. bandwidth: 86.4 GB/s) and the RTX 3070 (CUDA version 11.5, max. bandwidth: 448 GB/s) achieved with the respective level of optimisation. \relax }}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Benchmarks}{20}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Parallelisation parameters}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Scaling of the performance towards larger and smaller array sizes}{20}{}\protected@file@percent }
\newlabel{fig_num_threads}{{\caption@xref {fig_num_threads}{ on input line 11}}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  Benchmarks of the optimised kernel for \( 2^{27} \) 32-bit integers on a RTX 3070 (black) and A100 (red) for different number of threads per block. While the A100 outperformes the RTX 3070 as expected, they show they same characteristic in respect to the number of threads per block. The optimum lies somewhere between 128 and 256 threads. \relax }}{21}{}\protected@file@percent }
\citation{programming_guide}
\abx@aux@cite{0}{programming_guide}
\abx@aux@segm{0}{0}{programming_guide}
\newlabel{fig_scaling}{{\caption@xref {fig_scaling}{ on input line 33}}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  Performance of the reduction kernel over increasing 32-bit array sizes for a RTX 3070 and an A100. All points represent the mean of 1000 measurements. One can see nicely the transition to linear scaling. \relax }}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Dependence of the performance on the datatype}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{22}{}\protected@file@percent }
\newlabel{fig_datatypes}{{\caption@xref {fig_datatypes}{ on input line 55}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Performance of the reduction kernel for several datatypes over an array with \( 2^{28} \) elements on a RTX 3070 (black) and an A100 (red). Each data point reflects the mean of 1000 measurements. The execution time is proportional with the size of the datatype in the case of the RTX 3070, while for the A100 the larger datatypes are slightly more efficient in terms of bandwidth. \relax }}{23}{}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{44FDF0A31ED9437695B4E2CA1F4BDAE0}
\abx@aux@defaultrefcontext{0}{Harris}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Intel4004}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{programming_guide}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Rupp}{anyt/global//global/global}
\abx@aux@defaultlabelprefix{0}{Harris}{}
\abx@aux@defaultlabelprefix{0}{Intel4004}{}
\abx@aux@defaultlabelprefix{0}{programming_guide}{}
\abx@aux@defaultlabelprefix{0}{Rupp}{}
\gdef \@abspage@last{24}
